{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4a3a5f",
   "metadata": {
    "id": "Q8keQOCGZG2p"
   },
   "source": [
    "# 035. 네이버 영화평 감성 분류\n",
    "\n",
    "- 한글 형태소 분석기 Okt 사용 전처리  \n",
    "\n",
    "- Keras Tokenizer, pad_sequences 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699e1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98248e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae6f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07738926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7db5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8475bd",
   "metadata": {
    "id": "HEBq5p1xKRTL"
   },
   "source": [
    "### 훈련 시간을 감안하여 data size 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d74e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 50,000개의 샘플을 무작위로 선택 (재현성을 위해 random_state=1 사용)\n",
    "# 테스트 데이터에서 5,000개의 샘플을 무작위로 선택 (재현성을 위해 random_state=1 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66887bbe",
   "metadata": {
    "id": "k9Unf2JKZG21"
   },
   "source": [
    "**null value 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e5b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3794f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5063c09c",
   "metadata": {
    "id": "-yBDcodKM-fb"
   },
   "source": [
    "### okt.morphs()\n",
    "\n",
    "- 텍스트를 형태소 단위로 나눈다. 옵션으로는 norm과 stem이 있다\n",
    "- stem은 각 단어에서 어간을 추출하는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589d2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a310ff",
   "metadata": {
    "id": "Y-3baFyeZG3H"
   },
   "source": [
    "## Text Data 전처리\n",
    "\n",
    "**한글 문자가 아닌 것 모두 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence, remove_stopwords=True):\n",
    "    # 불용어 리스트 정의 (현재는 빈 리스트로 설정)\n",
    "    # stop_words = set(['에', '은', '는', '이', '가', '그리고', '것', '들', '수', '등', '로', '을', '를', '만', '도', '아', '의', '그', '다'])\n",
    "    # 개행문자 제거\n",
    "    # 한글 외의 모든 문자 제거\n",
    "    # 형태소 분석 및 어간 추출\n",
    "    # 불용어 제거 옵션이 True인 경우, 불용어 리스트에 포함되지 않은 토큰만 선택\n",
    "    # 전처리된 문장 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 문장과 레이블을 저장할 리스트 초기화\n",
    "# 테스트 문장과 레이블을 저장할 리스트 초기화\n",
    "# 시작 시간 기록\n",
    "# 훈련 데이터 전처리\n",
    "    # 10,000개마다 진행 상황 출력\n",
    "    # 문장 전처리 수행\n",
    "    # 전처리 후 문장이 비어있지 않으면 리스트에 추가\n",
    "# 테스트 데이터 전처리\n",
    "    # 1,000개마다 진행 상황 출력\n",
    "    # 문장 전처리 수행\n",
    "    # 전처리 후 문장이 비어있지 않으면 리스트에 추가\n",
    "# 전처리에 걸린 총 시간 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886d905",
   "metadata": {
    "id": "lV62dOYhRFAI"
   },
   "source": [
    "## train_labels, test_labels  list를 numpy array 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5becd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 레이블을 numpy 배열로 변환\n",
    "# 테스트 레이블을 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a347b4e",
   "metadata": {
    "id": "cYuPSbBrRbpn"
   },
   "source": [
    "## train_sentences, test_sentences text 를 sequence 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291085dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전의 최대 크기 설정\n",
    "# Tokenizer 객체 생성 (최대 단어 수 지정 및 OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 훈련 문장에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 훈련 문장들을 시퀀스로 변환\n",
    "# 테스트 문장들을 시퀀스로 변환\n",
    "# 첫 번째 시퀀스 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd406719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시퀀스와 테스트 시퀀스의 길이를 리스트로 변환하여 합친 후 히스토그램으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시퀀스를 패딩 처리 (최대 길이를 15로 설정, 'post' 방식으로 잘라내고 패딩)\n",
    "# 테스트 시퀀스를 패딩 처리 (최대 길이를 15로 설정, 'post' 방식으로 잘라내고 패딩)\n",
    "# 첫 번째 패딩된 시퀀스 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330c778",
   "metadata": {
    "id": "_XW692THZG3f"
   },
   "source": [
    "### sequence 를 다시 문장으로 역변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ff59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 인덱스를 키로, 단어를 값으로 하는 사전 생성\n",
    "# 시퀀스를 문장으로 디코딩하는 함수\n",
    "def decode_sentence(sequence):\n",
    "    # 시퀀스의 각 인덱스를 단어로 변환하여 리스트로 만들고, 이를 공백으로 연결하여 문자열로 반환\n",
    "# 다섯 번째 패딩된 훈련 시퀀스를 디코딩하여 출력\n",
    "# 다섯 번째 훈련 문장 원본 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfc582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424697f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc6730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16502fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c991d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_text = ['오랜만에 접한 수작']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13af69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a53178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acb443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6f31ba4",
   "metadata": {
    "id": "SPbB4Vu5ZG3u"
   },
   "source": [
    "## Embedding Layer 시각화\n",
    "\n",
    "- Embedding projector https://projector.tensorflow.org/  를 이용하여 word embedding 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 첫 번째 레이어 (임베딩 레이어) 가져오기\n",
    "# 임베딩 레이어의 가중치 가져오기\n",
    "# 임베딩 가중치의 형태 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622e41e",
   "metadata": {
    "id": "Z6UmmFaKZG3x"
   },
   "source": [
    "### embedding layer 의 weight 를 disk 에 write. Embedding projector 사용을 위해 embedding vector file 과 단어가 들어 있는 meta data file 로 구분하여 upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a7d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터와 메타데이터를 저장할 파일 열기 (UTF-8 인코딩 사용)\n",
    "# 단어 인덱스 1부터 999까지 반복\n",
    "    # 단어 인덱스를 단어로 변환 (인덱스가 없으면 '?' 반환)\n",
    "    # 해당 단어의 임베딩 벡터 가져오기\n",
    "    # 메타데이터 파일에 단어 쓰기\n",
    "    # 벡터 파일에 임베딩 벡터 쓰기 (탭으로 구분)\n",
    "# 파일 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784a423",
   "metadata": {
    "id": "1bjh-IuVZG3z"
   },
   "source": [
    "## Embedding 결과 확인\n",
    "\n",
    "[Embedding Projector](https://projector.tensorflow.org) 에 접속하여 embedding 의 품질 확인\n",
    "\n",
    "Google Colab 의 경우 local PC 로 download 받아 Embedding Projector 에 upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302f403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
