{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjogPl1KL4-"
   },
   "source": [
    "# 037. Tokenizer Training from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_E = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'I was born in Korea and graduaged University in USA.',\n",
    "]\n",
    "\n",
    "sentences_K = [\n",
    "    \"코로나가 심하다\",\n",
    "    \"코비드-19가 심하다\",\n",
    "    '아버지가방에들어가신다',\n",
    "    '아버지가 방에 들어가신다',\n",
    "    '너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Keras 기본 Tokenizer - rule-based\n",
    "- 공백 또는 구둣점으로 분리  \n",
    "- 영어 단어별로 띄어쓰기가 철저히 지켜지는 언어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'in': 6, 'cat': 7, 'you': 8, 'was': 9, 'born': 10, 'korea': 11, 'and': 12, 'graduaged': 13, 'university': 14, 'usa': 15}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')  # 빈도수 상위 100 개로 구성\n",
    "\n",
    "tokenizer.fit_on_texts(sentences_E)           \n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, '심하다': 2, '코로나가': 3, '코비드': 4, '19가': 5, '아버지가방에들어가신다': 6, '아버지가': 7, '방에': 8, '들어가신다': 9, '너무너무너무는': 10, '나카무라세이코가': 11, '불러': 12, '크게': 13, '히트한': 14, '노래입니다': 15}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')  # 빈도수 상위 100 개로 구성\n",
    "\n",
    "tokenizer.fit_on_texts(sentences_K)           \n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 단어 사전 기반 한국어 tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQKMuFq7MmJ6",
    "outputId": "53da2513-0534-4454-d76a-e316f69bb94d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코로나', '가', '심하다']\n",
      "['코', '비드', '-', '19', '가', '심하다']\n",
      "['아버지', '가방', '에', '들어가신다']\n",
      "['아버지', '가', '방', '에', '들어가신다']\n",
      "['너무', '너무', '너', '무', '는', '나카무라', '세이', '코', '가', '불러', '크게', '히트', '한', '노래', '입니다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "temp_X = []\n",
    "for sent in sentences_K:\n",
    "    temp_X.append(okt.morphs(sent))\n",
    "    print(okt.morphs(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, '가': 2, '심하다': 3, '코': 4, '아버지': 5, '에': 6, '들어가신다': 7, '너무': 8, '코로나': 9, '비드': 10, '-': 11, '19': 12, '가방': 13, '방': 14, '너': 15, '무': 16, '는': 17, '나카무라': 18, '세이': 19, '불러': 20, '크게': 21, '히트': 22, '한': 23, '노래': 24, '입니다': 25}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')  # 빈도수 상위 100 개로 구성\n",
    "\n",
    "tokenizer.fit_on_texts(temp_X)           \n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRtVftAuMmJ8"
   },
   "source": [
    "### 단, Okt 사전에 미등록된 단어의 경우 정확한 tokenizing 이 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUpB1dYWMmJ8",
    "outputId": "a83744b2-3317-429d-92e8-1eeeee5a7d9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('너무', 'Adverb'),\n",
       " ('너무', 'Adverb'),\n",
       " ('너', 'Modifier'),\n",
       " ('무', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('나카무라', 'Noun'),\n",
       " ('세이', 'Noun'),\n",
       " ('코', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('불러', 'Verb'),\n",
       " ('크게', 'Noun'),\n",
       " ('히트', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('노래', 'Noun'),\n",
       " ('입니다', 'Adjective')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos('너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Cq9CJhJMmJ9"
   },
   "source": [
    "예를 들어 `너무너무너무`와 `나카무라세이코`는 하나의 단어이지만, okt 사전에 등록되어 있지 않아 여러 개의 복합단어로 나뉘어집니다. 이러한 문제를 해결하기 위하여 형태소 분석기와 품사 판별기들은 사용자 사전 추가 기능을 제공합니다. 사용자 사전을 추가하여 모델의 vocabulary 를 풍부하게 만드는 것은 사용자의 몫입니다.\n",
    "\n",
    "1. okt 공식 문서를 참고해서 사용사 사전을 추가.\n",
    "2. okt를 패키징하고, konlpy에서 사용할 수 있도록 konlpy/java 경로에 jar 파일을 복사.\n",
    "3. 기존에 참고하고 있던 okt.jar 대신 새로운 okt.jar를 사용하도록 설정.\n",
    "4. konlpy 소스 경로를 import 해서 형태소 분석."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NSFEmKRMmKB"
   },
   "source": [
    "# 3. Google SentencePiece Tokenizer\n",
    "\n",
    "- NAVER Movie rating data 를 이용한 sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gk5gZcQeMmKB",
    "outputId": "656513b4-4dbd-4baf-af80-205912058ed5"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-9k6x1ZMmKC",
    "outputId": "a8e66283-6a31-40ab-c9c5-08b48f8c7e12"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "DATA_TRAIN_PATH = tf.keras.utils.get_file(\"ratings_train.txt\", \n",
    "        \"https://github.com/ironmanciti/NLP_lecture/raw/master/data/naver_movie/ratings_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn-WA_c6MmKC"
   },
   "source": [
    "- pandas.read_csv에서 quoting = 3으로 설정해주면 인용구(따옴표)를 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "5q_gXlchMmKD",
    "outputId": "aa1e9b02-f230-4d41-d721-7ebbb248ac73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_TRAIN_PATH, sep='\\t', quoting=3)\n",
    "\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42U-cU9lMmKD",
    "outputId": "c81a81fb-38e1-41ea-cb82-105ac181d6ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3Bm3wxwMmKD",
    "outputId": "beb514e9-acc9-4136-d9f8-f814880b3dab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149995, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h4yXblIMmKE"
   },
   "source": [
    "## 학습을 위해 text 를 따로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "U8HVwXaQMmKE"
   },
   "outputs": [],
   "source": [
    "with open('./nsmc.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_data.document.values:\n",
    "        try:\n",
    "            f.write(line + '\\n')\n",
    "        except:\n",
    "            print(\"write error ---> \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WML97jwWMmKE",
    "outputId": "e7c06a5a-85e1-4436-d7a2-8b228ab3c64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149996\n",
      "아 더빙.. 진짜 짜증나네요 목소리\n"
     ]
    }
   ],
   "source": [
    "#write 가 잘 되었는지 확인\n",
    "with open('./nsmc.txt', 'r', encoding='utf-8') as f:\n",
    "    nsmc_txt = f.read().split('\\n')\n",
    "    \n",
    "print(len(nsmc_txt))\n",
    "print(nsmc_txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ENQL0fs5MmKF",
    "outputId": "83ddbc05-0a50-45bd-9e3d-aeb4e8d5a872"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--input=nsmc.txt --model_prefix=nsmc --vocab_size=30000'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'nsmc.txt'\n",
    "vocab_size = 30000\n",
    "prefix = 'nsmc'\n",
    "\n",
    "templates = '--input={} --model_prefix={} --vocab_size={}'\n",
    "cmd = templates.format(input_file, prefix, vocab_size)\n",
    "cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IcsoxuvMmKF"
   },
   "source": [
    "### sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WaYRD7mjMmKF"
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zvm9kadMmKG",
    "outputId": "c9aca3ab-f92c-4f7c-ec7f-f07a64ff0725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBl78txHMmKG",
    "outputId": "8e38cc95-bf2d-4247-a376-bd7a90c247ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n",
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나네요', '▁목소리']\n",
      "[52, 752, 5, 25, 16019, 1401] \n",
      "\n",
      "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "['▁흠', '...', '포스터보고', '▁초딩영화', '줄', '....', '오버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "[1239, 6, 12819, 18669, 423, 47, 18184, 416, 1133, 6423, 1083, 417] \n",
      "\n",
      "너무재밓었다그래서보는것을추천한다\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보는것', '을', '추천', '한다']\n",
      "[17, 600, 21569, 670, 2750, 10787, 14, 2321, 296] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in train_data.document.values[:3]:\n",
    "    print(t)\n",
    "    print(sp.encode_as_pieces(t))\n",
    "    print(sp.encode_as_ids(t), '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG8_6rToMmKG",
    "outputId": "b8e10e26-8e2e-401c-b3f3-8c77f0e06963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코로나가 심하다\n",
      "['▁코', '로', '나', '가', '▁심하다']\n",
      "[1359, 29, 33, 13, 5383]\n",
      "\n",
      "코비드-19가 심하다\n",
      "['▁코', '비', '드', '-', '19', '가', '▁심하다']\n",
      "[1359, 334, 277, 282, 3863, 13, 5383]\n",
      "\n",
      "아버지가방에들어가신다\n",
      "['▁아버지가', '방', '에', '들어가', '신', '다']\n",
      "[6161, 618, 16, 13140, 267, 23]\n",
      "\n",
      "아버지가 방에 들어가신다\n",
      "['▁아버지가', '▁방', '에', '▁들어가', '신', '다']\n",
      "[6161, 1569, 16, 3870, 267, 23]\n",
      "\n",
      "너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다\n",
      "['▁너무너무너무', '는', '▁나카', '무라', '세', '이', '코가', '▁불러', '▁크게', '▁히트', '한', '▁노래', '입니다']\n",
      "[14344, 12, 17264, 10088, 262, 10, 13095, 3392, 1846, 10169, 30, 765, 228]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in sentences_K:\n",
    "    pieces = sp.encode_as_pieces(line)\n",
    "    ids = sp.encode_as_ids(line)\n",
    "    print(line)\n",
    "    print(pieces)\n",
    "    print(ids)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "037_SentencePiece_Tokenizer_Training_From_Scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
