{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6c2edb",
   "metadata": {
    "id": "bqRzQjEMbmvy"
   },
   "source": [
    "# 010. Vectorization of Statement (문장의 vector 화)\n",
    "\n",
    "- BOW (Bag of Words)\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency)  \n",
    "- Word Embedding - Keras word API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabe0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "740cdc58",
   "metadata": {
    "id": "wVoHTfTZbmv0"
   },
   "source": [
    "## 1. Bag of Word (BOW)\n",
    "\n",
    "CountVectorizer는 Python의 scikit-learn 라이브러리에 포함된 클래스로, 텍스트 데이터의 토큰화(tokenization)와 단어 빈도 수를 기반으로 하는 피처 벡터(feature vector)를 생성하는 데 사용됩니다. 이 클래스는 자연어 처리(Natural Language Processing, NLP)와 텍스트 마이닝에서 널리 사용되며, 주요 기능은 다음과 같습니다:\n",
    "\n",
    "토큰화(Tokenization): 문장이나 문서를 개별 단어나 표현으로 분할합니다.\n",
    "\n",
    "단어 빈도 계산(Word Frequency Counting): 각 단어가 문서 내에서 나타나는 빈도를 계산합니다.\n",
    "\n",
    "피처 벡터 생성(Feature Vector Creation): 각 문서를 단어의 빈도를 나타내는 벡터로 변환합니다. 이 벡터는 머신러닝 알고리즘에 입력으로 사용될 수 있습니다.\n",
    "\n",
    "사전 구축(Vocabulary Building): 모든 문서에서 사용된 모든 단어의 사전을 만듭니다.\n",
    "\n",
    "CountVectorizer를 사용하면 텍스트 데이터를 수치적으로 분석할 수 있으며, 이는 감정 분석, 주제 모델링, 문서 분류와 같은 다양한 NLP 응용 프로그램에서 중요한 단계입니다. 예를 들어, 스팸 메일 분류, 문서 군집화, 텍스트 기반 추천 시스템 등에 사용됩니다.\n",
    "\n",
    "- CountVectorizer 주요 파라미터  \n",
    "    - min_df : vocabulary 에 포함할 최소 발생 빈도\n",
    "    - ngram_range : (1, 1) - unigram only, (1, 2) - unigram + bigram\n",
    "    - max_features : top max_features 만으로 vocabulary 구성\n",
    "    - token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b : unocode 영수자 2 글자 이상만 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d3f5f",
   "metadata": {
    "id": "00iVGIN7bmv1"
   },
   "source": [
    "## Text vs token Matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32507a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer 객체 생성\n",
    "# sentences 데이터에 대한 피처 변환 수행\n",
    "# sentences는 분석할 텍스트 데이터의 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567ca14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features 객체를 NumPy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56381e2f",
   "metadata": {
    "id": "kLr_wKbibmv2"
   },
   "source": [
    "### features 의 단어 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer를 통해 추출한 피처(단어) 이름들을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화된 문장과 피처 이름을 이용해 DataFrame 생성\n",
    "# 데이터프레임의 인덱스 이름 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2a6f2",
   "metadata": {
    "id": "WSCEeO7Abmv2"
   },
   "source": [
    "## 2. TF-IDF\n",
    "\n",
    "- TF-IDF(Term Frequency - Inverse Document Frequency)  \n",
    "\n",
    "TF-IDF는 단어의 빈도와 그 단어가 드물게 나타나는 문서에 더 높은 가중치를 부여하는 방식으로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb368f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer 객체 생성\n",
    "# sentences 데이터에 대한 TF-IDF 기반 피처 변환 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9f7ce",
   "metadata": {
    "id": "7_YNzGsIbmv3"
   },
   "source": [
    "## Text vs tf-idf Matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c53b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 피처 객체를 NumPy 배열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977582eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer를 통해 추출한 피처(단어) 이름들을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화된 문장과 피처 이름을 이용해 DataFrame 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed9472",
   "metadata": {
    "id": "GJIYRQeEbmv3"
   },
   "source": [
    "# 3. keras word encoding\n",
    "\n",
    "- keras  API 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844c078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01b3b4b6",
   "metadata": {
    "id": "F_3RVyplbmv3"
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9743724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장으로 부터 상위 100 개 단어로 vocabulary 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882cf06",
   "metadata": {
    "id": "Mkinjq-Xbmv3"
   },
   "source": [
    "## Word Index Vocabulary 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95068ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences에 포함된 문장들을 기반으로 단어의 토큰화를\n",
    "# 수행하며, 각 단어에 고유한 인덱스를 할당\n",
    "# 각 단어에 부여된 고유 인덱스 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542528d2",
   "metadata": {
    "id": "lV07Led4bmv4"
   },
   "source": [
    "## text 의 sentence 변환 및 paddding\n",
    "\n",
    "- texts_to_sequences: text list 내의 각 text 를 수열 (sequence of integers) 로 convert\n",
    "\n",
    "\n",
    "    - 입력 : text (strings) list\n",
    "    - 반환 : sequence list\n",
    "    \n",
    "- pad_sequences: 동일한 길이로 sequence 를 zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50481679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences 데이터를 시퀀스로 변환\n",
    "# 시퀀스에 패딩 적용 (문장의 뒤쪽을 패딩하고, 필요시 뒤쪽을 잘라냄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1241dea",
   "metadata": {
    "id": "sMjZr__Wbmv4"
   },
   "source": [
    "### sequenced sentence 를 word sentence 로 환원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences 리스트에 있는 각 시퀀스를 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e7005",
   "metadata": {
    "id": "2Gs4XtP4bmv4"
   },
   "source": [
    "### One-Hot-Encoding 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩된 시퀀스를 원-핫 인코딩으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee68b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
